{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import pickle\n",
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger,\n",
    "    NamesExtractor,\n",
    "    Doc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)\n",
    "ner_tagger = NewsNERTagger(emb)\n",
    "names_extractor = NamesExtractor(morph_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word:\n",
    "    token: str\n",
    "    lemma: str\n",
    "    pos: str\n",
    "\n",
    "    def __init__(self, token: str, lemma: str, pos: str) -> None:\n",
    "        self.token = token\n",
    "        self.lemma = lemma if lemma is not None else ''\n",
    "        self.pos = pos\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.token + ' ' + self.lemma + ' ' + self.pos\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return self.token + ' ' + self.lemma + ' ' + self.pos\n",
    "\n",
    "class Text:\n",
    "    source: str\n",
    "    text: str\n",
    "    words: List[Word]\n",
    "    current: int\n",
    "\n",
    "    def __init__(self, source: str, text: str) -> None:\n",
    "        self.current = -1\n",
    "        self.source = source\n",
    "        self.text = text\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        self.current += 1\n",
    "        if self.current < len(self.words):\n",
    "            return self.words[self.current]\n",
    "        self.current = -1\n",
    "        raise StopIteration\n",
    "\n",
    "    def parse_text(self) -> None:\n",
    "        doc = Doc(self.text)\n",
    "        doc.segment(segmenter)\n",
    "        doc.tag_morph(morph_tagger)\n",
    "        for t in doc.tokens:\n",
    "            t.lemmatize(morph_vocab)\n",
    "        self.words = [Word(t.text.lower(), t.lemma, t.pos) for t in doc.tokens if t.pos != 'PUNCT']\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.source + ' ' + self.text\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return self.source + ' ' + self.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from sympy import prime, primerange\n",
    "from collections import defaultdict\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Searcher:\n",
    "    pos_matrix: np.ndarray\n",
    "    lemmas_matrix: np.ndarray\n",
    "    tokens_matrix: np.ndarray\n",
    "\n",
    "    pos_vocab: dict\n",
    "    lemmas_vocab: dict\n",
    "    tokens_vocab: dict\n",
    "\n",
    "    max_len: int\n",
    "    prime_ids: np.ndarray\n",
    "\n",
    "    articles: list\n",
    "        \n",
    "    conv: np.ndarray\n",
    "    kernel: np.ndarray\n",
    "    \n",
    "    query_status: defaultdict\n",
    "\n",
    "    morph: MorphAnalyzer\n",
    "\n",
    "    def __init__(self, articles: list) -> None:\n",
    "        self.articles = articles\n",
    "        self.max_len = len(max(self.articles, key=lambda x: len(x.words)).words)\n",
    "        self.morph = MorphAnalyzer()\n",
    "        self.prime_ids = np.array([list(primerange(1, prime(self.max_len)+1)),\n",
    "        list(primerange(prime(self.max_len)+1, prime(2 * self.max_len)+1)),\n",
    "        list(primerange(prime(2 * self.max_len)+1 ,prime(3 * self.max_len)+1))])\n",
    "#         in order of  [POS, lemma, token]\n",
    "        self.pos_vocab = {x[1]: x[0] for x in enumerate(list(set([word.pos\n",
    "        for words in self.articles for word in words])))}\n",
    "        self.lemmas_vocab = {x[1]: x[0] for x in enumerate(list(set([word.lemma\n",
    "        for words in self.articles for word in words])))}\n",
    "        self.tokens_vocab = {x[1]: x[0] for x in enumerate(list(set([word.token\n",
    "        for words in self.articles for word in words])))}\n",
    "\n",
    "        self.pos_matrix = np.zeros((len(self.articles), len(self.pos_vocab)))\n",
    "        self.lemmas_matrix = np.zeros((len(self.articles), len(self.lemmas_vocab)))\n",
    "        self.tokens_matrix = np.zeros((len(self.articles), len(self.tokens_vocab)))\n",
    "        for i, sent in enumerate(self.articles):\n",
    "            for j, t in enumerate(sent):\n",
    "                self.pos_matrix[i][self.pos_vocab[t.pos]]\\\n",
    "                += math.log(self.prime_ids[0][j])\n",
    "                self.lemmas_matrix[i][self.lemmas_vocab[t.lemma]]\\\n",
    "                += math.log(self.prime_ids[1][j])\n",
    "                self.tokens_matrix[i][self.tokens_vocab[t.token]]\\\n",
    "                += math.log(self.prime_ids[2][j])\n",
    "        \n",
    "        \n",
    "    def parse_query(self, query) -> (defaultdict, np.ndarray, np.ndarray, np.ndarray):\n",
    "        pos_query = np.zeros((len(self.pos_vocab), 1))\n",
    "        lemmas_query = np.zeros((len(self.lemmas_vocab), 1))\n",
    "        tokens_query = np.zeros((len(self.tokens_vocab), 1))\n",
    "        query_status = defaultdict(dict)\n",
    "        toks = query.split()\n",
    "        if len(toks) == 1 and toks[0].find('+') < 0:\n",
    "            query_status['simple'] = True\n",
    "        for i, tok in enumerate(toks):\n",
    "            split_toks = tok.split('+')\n",
    "            for s_tok in split_toks:\n",
    "                if s_tok.startswith('''\"'''):\n",
    "                    query_status['token'][i] = set([s_tok.strip('''\\\"''')])\n",
    "                    try:\n",
    "                        tokens_query[self.tokens_vocab[s_tok.strip('''\"''')]][0] = i + 1\n",
    "                    except KeyError:\n",
    "                        query_status['invalid'] = 'Token %s is not found' % s_tok\n",
    "                        return query_status, None, None, None\n",
    "                elif s_tok in self.pos_vocab.keys():\n",
    "                    query_status['POS'][i] =  set([s_tok])\n",
    "                    pos_query[self.pos_vocab[s_tok]][0] = i + 1\n",
    "                    print(np.nonzero(pos_query))\n",
    "                else:\n",
    "                    ana = self.morph.parse(s_tok)\n",
    "                    poss_lemmas = set([x.normal_form for x in ana])\n",
    "                    valid = False\n",
    "                    query_status['lemma'][i] = poss_lemmas\n",
    "                    for lemma in poss_lemmas:\n",
    "                        try:\n",
    "                            lemmas_query[self.lemmas_vocab[lemma]][0] = i + 1\n",
    "                            print(np.nonzero(lemmas_query))\n",
    "                            valid = True\n",
    "                        except KeyError:\n",
    "                            continue\n",
    "                    if not valid:\n",
    "                        query_status['invalid'] =\\\n",
    "                        'Lemmas %s are either not valid POS tags or not found' % ' '.join(poss_lemmas)\n",
    "                        return query_status, None, None, None\n",
    "        return query_status, pos_query, lemmas_query, tokens_query\n",
    "    \n",
    "    def display_results(self, rel) -> dict:\n",
    "        result = {}\n",
    "        if not self.query_status['simple']:\n",
    "            rel = self.brute_force(rel)\n",
    "        if len(rel) == 0:\n",
    "            return {-1: 'Nothing found'}\n",
    "        for n, idx in enumerate(rel):\n",
    "            sent = self.articles[idx]\n",
    "            result[n] = [idx, sent.source, sent.text]\n",
    "        return result\n",
    "    \n",
    "    def brute_force(self, rel) -> np.ndarray:\n",
    "        real_rel = []\n",
    "        for n, idx in enumerate(rel):\n",
    "            sent = self.articles[idx].words\n",
    "            for i in range(len(sent) + 1 - self.kernel.shape[1]):\n",
    "                valid_m = True\n",
    "                for j, w in self.query_status['POS'].items():\n",
    "                    if sent[i+j].pos not in w:\n",
    "                        valid_m = False\n",
    "                for j, w in self.query_status['lemma'].items():\n",
    "                    if sent[i+j].lemma not in w:\n",
    "                        valid_m = False\n",
    "                for j, w in self.query_status['token'].items():\n",
    "                    if sent[i+j].token not in w:\n",
    "                        valid_m = False\n",
    "                if valid_m:\n",
    "                    real_rel.append(idx)\n",
    "                    break\n",
    "        return real_rel\n",
    "            \n",
    "        \n",
    "    \n",
    "    def process_query(self, query) -> list:\n",
    "        def _simple_search(query, matrix) -> np.ndarray:\n",
    "            res = np.argwhere(matrix @ query > 0)\n",
    "            return res[:, 0]\n",
    "        \n",
    "        def _create_kernel(query_status) -> np.ndarray:\n",
    "            kernel = np.zeros((3, 3))\n",
    "            for row, term in {0: 'POS', 1: 'lemma', 2: 'token'}.items():\n",
    "                if query_status[term]:\n",
    "                    for i in query_status[term].keys():\n",
    "                        kernel[row][i] = i+1\n",
    "            return kernel\n",
    "        \n",
    "        def _inverted_prime_convolution(primes, kernel) -> np.ndarray:\n",
    "            kernel = np.delete(kernel, np.argwhere(np.all(kernel[..., :] == 0, axis=0)), axis=1)\n",
    "            fin = primes.shape[1] + 1 - kernel.shape[1]\n",
    "            res = np.ones(fin)\n",
    "            for i in range(fin):\n",
    "                for j in range(kernel.shape[1]):\n",
    "                    res[i] /= (math.pow(primes[0][i + j], kernel[0][j])\\\n",
    "                    * math.pow(primes[1][i + j], kernel[1][j])\\\n",
    "                    * math.pow(primes[2][i + j], kernel[2][j]))\n",
    "            return res\n",
    "        \n",
    "        def _find_integers(prime_mapping) -> np.ndarray:\n",
    "            e = math.pow(10, -19)\n",
    "            mask = np.vectorize(lambda x: np.isclose(x, np.round(x), atol=e))(prime_mapping)\n",
    "            return np.nonzero(np.sum(mask, axis=1))[0]\n",
    "        \n",
    "        self.query_status, pos_query, lemmas_query, tokens_query = self.parse_query(query)\n",
    "        if self.query_status['invalid']:\n",
    "            return {-1: self.query_status['invalid']}\n",
    "        if self.query_status['simple']:\n",
    "            if self.query_status['POS']:\n",
    "                rel = _simple_search(pos_query, self.pos_matrix)\n",
    "            elif self.query_status['lemma']:\n",
    "                rel = _simple_search(lemmas_query, self.lemmas_matrix)\n",
    "            else:\n",
    "                rel = _simple_search(tokens_query, self.tokens_matrix)\n",
    "        else:\n",
    "            char_vec = np.ones(len(self.articles))\n",
    "            self.kernel = _create_kernel(self.query_status)\n",
    "            self.conv = _inverted_prime_convolution(self.prime_ids, self.kernel)\n",
    "            if self.query_status['POS']:\n",
    "                char_vec *= np.around(np.exp(self.pos_matrix @ pos_query)).flatten()\n",
    "            if self.query_status['lemma']:\n",
    "                char_vec *= np.around(np.exp(self.lemmas_matrix @ lemmas_query)).flatten()\n",
    "            if self.query_status['token']:\n",
    "                char_vec *= np.around(np.exp(self.tokens_matrix @ tokens_query)).flatten()\n",
    "            print(char_vec[0])\n",
    "            prime_mapping = char_vec.reshape((-1, 1)) @ self.conv.reshape((1, -1))\n",
    "            rel = _find_integers(prime_mapping)\n",
    "        print(self.display_results(rel))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"articles_parsed_final\", \"rb\") as f:\n",
    "        a = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher = Searcher(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.36818719  7.24636808  0.         11.38150613  1.94591015  3.4339872\n",
      "  3.76120012  0.          0.         14.82901547  0.          0.\n",
      "  0.          3.61091791  5.64190707  3.36729583]\n"
     ]
    }
   ],
   "source": [
    "with open('corpus', 'wb') as f:\n",
    "    pickle.dump(searcher, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
