{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import pickle\n",
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger,\n",
    "    NamesExtractor,\n",
    "    Doc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)\n",
    "ner_tagger = NewsNERTagger(emb)\n",
    "names_extractor = NamesExtractor(morph_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классы, которыми мы представляем тексты (мы считаем предложение за \"текст\"), в котором есть метод, который умеет делать разбор своего текста и выдавать список слов, которые представлены отдельным классом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word:\n",
    "    token: str\n",
    "    lemma: str\n",
    "    pos: str\n",
    "\n",
    "    def __init__(self, token: str, lemma: str, pos: str) -> None:\n",
    "        self.token = token\n",
    "        self.lemma = lemma\n",
    "        self.pos = pos\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.token + ' ' + self.lemma + ' ' + self.pos\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return self.token + ' ' + self.lemma + ' ' + self.pos\n",
    "\n",
    "class Text:\n",
    "    source: str\n",
    "    text: str\n",
    "    words: List[Word]\n",
    "\n",
    "    def __init__(self, source: str, text: str) -> None:\n",
    "        self.current = -1\n",
    "        self.source = source\n",
    "        self.text = text\n",
    "\n",
    "    def parse_text(self) -> None:\n",
    "        doc = Doc(self.text)\n",
    "        doc.segment(segmenter)\n",
    "        doc.tag_morph(morph_tagger)\n",
    "        for t in doc.tokens:\n",
    "            t.lemmatize(morph_vocab)\n",
    "        self.words = [Word(t.text.lower(), t.lemma, t.pos) \\\n",
    "                for t in doc.tokens if t.pos != 'PUNCT']\n",
    "\n",
    "    def iter(self):\n",
    "        return self\n",
    "    \n",
    "    def next(self):\n",
    "        self.current += 1\n",
    "        if self.current < len(self.words):\n",
    "            return self.words[self.current]\n",
    "        self.current = -1\n",
    "        raise StopIteration\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.source + ' ' + self.text\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return self.source + ' ' + self.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прочитываю csv с предложениями, создаю список из объектов Text и делаю разбор предложений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"articles.csv\", \"r\") as f:\n",
    "    texts = f.readlines()\n",
    "del texts[0]\n",
    "for i in range(len(texts)):\n",
    "    texts[i] = texts[i].split(',\" ')\n",
    "    texts[i] = Text(texts[i][0], texts[i][1])\n",
    "    texts[i].parse_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция, чтобы сериализовать список с предложениями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_texts(texts: List[Text], file: str) -> None:\n",
    "    with open(file, \"wb\") as f:\n",
    "        # pickle.dump(texts[:30:3], f)\n",
    "        pickle.dump(texts, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция, чтобы десериализовать список предложений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_texts(filename: str) -> List[Text]:\n",
    "    with open(filename, \"rb\") as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это функция, которая нужна была нам, когда мы что-то добавляли в классы и надо было менять их у уже сериализованных текстов, чтобы все работало"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_classes(texts: List[Text]) -> None:\n",
    "    for t in texts:\n",
    "        t.__class__ = Text\n",
    "        for w in t.words:\n",
    "            w.__class__ = Word"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
