{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import pickle\n",
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger,\n",
    "    NamesExtractor,\n",
    "    Doc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)\n",
    "ner_tagger = NewsNERTagger(emb)\n",
    "names_extractor = NamesExtractor(morph_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word:\n",
    "    token: str\n",
    "    lemma: str\n",
    "    pos: str\n",
    "\n",
    "    def __init__(self, token: str, lemma: str, pos: str) -> None:\n",
    "        self.token = token\n",
    "        self.lemma = lemma if lemma is not None else ''\n",
    "        self.pos = pos\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.token + ' ' + self.lemma + ' ' + self.pos\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return self.token + ' ' + self.lemma + ' ' + self.pos\n",
    "\n",
    "class Text:\n",
    "    source: str\n",
    "    text: str\n",
    "    words: List[Word]\n",
    "\n",
    "    def __init__(self, source: str, text: str) -> None:\n",
    "        self.current = -1\n",
    "        self.source = source\n",
    "        self.text = text\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        self.current += 1\n",
    "        if self.current < len(self.words):\n",
    "            return self.words[self.current]\n",
    "        self.current = -1\n",
    "        raise StopIteration\n",
    "\n",
    "    def parse_text(self) -> None:\n",
    "        doc = Doc(self.text)\n",
    "        doc.segment(segmenter)\n",
    "        doc.tag_morph(morph_tagger)\n",
    "        for t in doc.tokens:\n",
    "            t.lemmatize(morph_vocab)\n",
    "        self.words = [Word(t.text.lower(), t.lemma, t.pos) for t in doc.tokens if t.pos != 'PUNCT']\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.source + ' ' + self.text\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return self.source + ' ' + self.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from sympy import prime, primerange\n",
    "from collections import defaultdict\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Searcher:\n",
    "    pos_matrix: np.ndarray\n",
    "    lemmas_matrix: np.ndarray\n",
    "    tokens_matrix: np.ndarray\n",
    "    \n",
    "    pos_vocab: dict\n",
    "    lemmas_vocab: dict\n",
    "    tokens_vocab: dict\n",
    "        \n",
    "    max_len: int\n",
    "    prime_ids: np.ndarray\n",
    "    \n",
    "    articles: list\n",
    "    \n",
    "    morph: MorphAnalyzer\n",
    "    \n",
    "    def __init__(self, articles: list) -> None:\n",
    "        self.articles = articles\n",
    "        self.max_len = len(max(self.articles, key=lambda x: len(x.words)).words)\n",
    "        self.morph = MorphAnalyzer()\n",
    "        self.prime_ids = np.array([list(primerange(1, prime(self.max_len)+1)), list(primerange(prime(self.max_len)+1 ,prime(2 * self.max_len)+1)), list(primerange(prime(2 * self.max_len)+1 ,prime(3 * self.max_len)+1))])\n",
    "#         in order of  [POS, lemma, token]\n",
    "        self.pos_vocab = {x[1]: x[0] for x in enumerate(list(set([word.pos for words in self.articles for word in words])))}\n",
    "        self.lemmas_vocab = {x[1]: x[0] for x in enumerate(list(set([word.lemma for words in self.articles for word in words])))}\n",
    "        self.tokens_vocab = {x[1]: x[0] for x in enumerate(list(set([word.token for words in self.articles for word in words])))}\n",
    "\n",
    "        self.pos_matrix = np.zeros((len(self.articles), len(self.pos_vocab)))\n",
    "        self.lemmas_matrix = np.zeros((len(self.articles), len(self.lemmas_vocab)))\n",
    "        self.tokens_matrix = np.zeros((len(self.articles), len(self.tokens_vocab)))\n",
    "        for i, sent in enumerate(self.articles):\n",
    "            for j, t in enumerate(sent):\n",
    "                self.pos_matrix[i][self.pos_vocab[t.pos]] += math.log(self.prime_ids[0][j])\n",
    "                self.lemmas_matrix[i][self.lemmas_vocab[t.lemma]] += math.log(self.prime_ids[1][j])\n",
    "                self.tokens_matrix[i][self.tokens_vocab[t.token]] += math.log(self.prime_ids[2][j])\n",
    "        \n",
    "        \n",
    "    def parse_query(self, query) -> (defaultdict, np.ndarray, np.ndarray, np.ndarray):\n",
    "        pos_query = np.zeros((len(self.pos_vocab), 1))\n",
    "        lemmas_query = np.zeros((len(self.lemmas_vocab), 1))\n",
    "        tokens_query = np.zeros((len(self.tokens_vocab), 1))\n",
    "        query_status = defaultdict(list)\n",
    "        toks = query.split()\n",
    "        if len(toks) == 1 and toks[0].find('+') < 0:\n",
    "            query_status['simple'] = True\n",
    "        for i, tok in enumerate(toks):\n",
    "            split_toks = tok.split('+')\n",
    "            for s_tok in split_toks:\n",
    "                if s_tok.startswith('''\"'''):\n",
    "                    query_status['token'].append(i)\n",
    "                    try:\n",
    "                        tokens_query[self.tokens_vocab[s_tok.strip('''\"''')]][0] = i + 1\n",
    "                    except KeyError:\n",
    "                        query_status['invalid'] = 'Token %s is not found' % s_tok\n",
    "                        return query_status, None, None, None\n",
    "                elif s_tok in self.pos_vocab.keys():\n",
    "                    query_status['POS'].append(i)\n",
    "                    pos_query[self.pos_vocab[s_tok]][0] = i + 1\n",
    "                else:\n",
    "                    query_status['lemma'].append(i)\n",
    "                    ana = self.morph.parse(s_tok)\n",
    "                    poss_lemmas = set([x.normal_form for x in ana])\n",
    "                    valid = False\n",
    "                    for lemma in poss_lemmas:\n",
    "                        try:\n",
    "                            lemmas_query[self.lemmas_vocab[lemma]][0] = i + 1\n",
    "                            valid = True\n",
    "                        except KeyError:\n",
    "                            continue\n",
    "                    if not valid:\n",
    "                        query_status['invalid'] = 'Lemmas %s are either not valid POS tags or not found' % ' '.join(poss_lemmas)\n",
    "                        return query_status, None, None, None\n",
    "        return query_status, pos_query, lemmas_query, tokens_query\n",
    "    \n",
    "    def display_results(self, rel) -> dict:\n",
    "        result = {}\n",
    "        if len(rel) == 0:\n",
    "            return {-1: 'Nothing found'}\n",
    "        for n, idx in enumerate(rel):\n",
    "            sent = self.articles[idx]\n",
    "            result[n] = [sent.source, sent.text]\n",
    "        return result\n",
    "        \n",
    "    \n",
    "    def process_query(self, query) -> list:\n",
    "        def _simple_search(query, matrix) -> np.ndarray:\n",
    "            res = np.argwhere(matrix @ query > 0)\n",
    "            return res[:, 0]\n",
    "        \n",
    "        def _create_kernel(query_status) -> np.ndarray:\n",
    "            kernel = np.zeros((3, 3))\n",
    "            for row, term in {0: 'POS', 1: 'lemma', 2: 'token'}.items():\n",
    "                if query_status[term]:\n",
    "                    for i in query_status[term]:\n",
    "                        kernel[row][i] = i+1\n",
    "            return kernel\n",
    "        \n",
    "        def _inverted_prime_convolution(primes, kernel) -> np.ndarray:\n",
    "            kernel = np.delete(kernel, np.argwhere(np.all(kernel[..., :] == 0, axis=0)), axis=1)\n",
    "            fin = primes.shape[1] + 1 - kernel.shape[1]\n",
    "            res = np.ones(fin)\n",
    "            for i in range(fin):\n",
    "                for j in range(kernel.shape[1]):\n",
    "                    res[i] /= (math.pow(primes[0][i + j], kernel[0][j]) * math.pow(primes[1][i + j], kernel[1][j]) * math.pow(primes[2][i + j], kernel[2][j]))\n",
    "            return res\n",
    "        \n",
    "        def _find_integers(prime_mapping) -> np.ndarray:\n",
    "            mask = np.vectorize(lambda x: abs(x - np.round(x)) < math.pow(10, -15))(prime_mapping)\n",
    "            return np.nonzero(np.sum(mask, axis=1))[0]\n",
    "        \n",
    "        query_status, pos_query, lemmas_query, tokens_query = self.parse_query(query)\n",
    "        if query_status['invalid']:\n",
    "            return {-1: query_status['invalid']}\n",
    "        if query_status['simple']:\n",
    "            if query_status['POS']:\n",
    "                rel = _simple_search(pos_query, self.pos_matrix)\n",
    "            elif query_status['lemma']:\n",
    "                rel = _simple_search(lemmas_query, self.lemmas_matrix)\n",
    "            else:\n",
    "                rel = _simple_search(tokens_query, self.tokens_matrix)\n",
    "        else:\n",
    "            char_vec = np.ones(len(self.articles))\n",
    "            kernel = _create_kernel(query_status)\n",
    "            convolution = _inverted_prime_convolution(self.prime_ids, kernel)\n",
    "            if query_status['POS']:\n",
    "                print(pos_query, 'p')\n",
    "                char_vec *= np.around(np.exp(self.pos_matrix @ pos_query)).flatten()\n",
    "            if query_status['lemma']:\n",
    "                char_vec *= np.around(np.exp(self.lemmas_matrix @ lemmas_query)).flatten()\n",
    "            if query_status['token']:\n",
    "                char_vec *= np.around(np.exp(self.tokens_matrix @ tokens_query)).flatten()\n",
    "            prime_mapping = char_vec.reshape((-1, 1)) @ convolution.reshape((1, -1))\n",
    "            rel = _find_integers(prime_mapping)\n",
    "        return json.dumps(self.display_results(rel), ensure_ascii=False).encode('utf8')\n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"articles_parsed\", \"rb\") as f:\n",
    "        a = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher = Searcher(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': ['https://tproger.ru/articles/kak-v-2012-godu-predstavljali-razrabotku-v-2020-godu/',\n",
       "  'Eivind Eklund: Мы можем ожидать примерно такие же изменения, как и с 2005.\\n'],\n",
       " '1': ['https://tproger.ru/articles/hakatony-kak-oni-ustroeny-kak-prohodjat-i-chem-otlichajutsja-ot-chempionatov-po-analizu-dannyh/',\n",
       "  'Цели проведения дата-чемпионатов для компаний такие же, как в случае с хакатонами.\\n'],\n",
       " '2': ['https://tproger.ru/articles/vremennaja-pochta/',\n",
       "  'Письмо тоже не заставило себя ждать:  Но с рекламой ситуация такая же, как у Temp-Mail.\\n'],\n",
       " '3': ['https://tproger.ru/articles/vzlom-wi-fi-sposoby-i-programmy/',\n",
       "  'Принцип примерно такой же.\\n'],\n",
       " '4': ['https://tproger.ru/articles/3-interesnye-i-poleznye-igry-dlja-programmistov/',\n",
       "  'Польза для программиста: так как игра по своей сути похожа на Factorio, то преимущества будут такими же.\\n'],\n",
       " '5': ['https://tproger.ru/articles/idei-dinamicheskogo-programmirovanija-odnomernye-zadachi-chast-1/',\n",
       "  'Попробуем применить такую же идею здесь: Запускаем функцию с\\xa0вычислением времени: 41-е число вычисляется 135 секунд.\\n'],\n",
       " '6': ['https://tproger.ru/articles/kak-pravilno-vybrat-tehnologicheskij-stek-dlja-svoego-proekta/',\n",
       "  'Из-за этого нет уверенности в том, что\\xa0 Vue.js будет таким же популярным, как React и Angular через 10 лет.\\n']}"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(searcher.process_query('''такой же'''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('corpus', 'wb') as f:\n",
    "    pickle.dump(searcher, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
